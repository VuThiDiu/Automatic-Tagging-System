# -*- coding: utf-8 -*-
"""AutomateTaggingImageClothes_update.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1LBiip2IJPSuDgjwKy6e3YedoMh6VCtDP
"""

import numpy as np
import pandas as pd
import os 
import glob
import matplotlib.pyplot as plt
from logging import error
from sklearn.utils import shuffle
import tensorflow as tf
from tensorflow.keras.utils import to_categorical
from PIL import Image
from keras.layers.convolutional import Conv2D
from keras.models import Sequential,Model,load_model
from tensorflow.keras.optimizers import SGD
from keras.layers import BatchNormalization, Lambda, Input, Dense, Convolution2D, MaxPooling2D, AveragePooling2D, ZeroPadding2D, Dropout, Flatten, merge, Reshape, Activation
from keras.layers.merge import Concatenate
from keras.preprocessing.image import ImageDataGenerator
from keras.callbacks import ModelCheckpoint
from keras.layers import Input

import numpy as np

dataset_folder_name = "E:\Crawler\change\data"
train_test_split = 0.8
image_width = image_height = 100

#tag predict 
os.chdir("E:\Crawler\change\data")
dataset_dict = {
    'category_id' : {
        0:'quần dài', 
        1:'quần short', 
        2:'váy liền',
        3:'áo phông', 
        4:'áo sơ mi',
        5:'áo nỉ',
        6:'áo khoác' 
    },
}


dataset_dict['category_alias'] = dict((g,i) for i, g in dataset_dict['category_id'].items())
# extracting the data from the dataset
def parse_dataset(dataset_path):

  """ 
  Use to extract information about our dataset. It does interate over all images 
  and return a DataFrame with the data (category, color, gender, isAdult) of all files.
  """
  def parse_into_from_file(path):
    """
    Parse information from a single file
    """
    try:
      filename = os.path.split(path)[1]
      filename = os.path.splitext(filename)[0]
      filename = filename.split(' ')[0]
      listname = filename.split('_')
      category,color,gender,isAdult = listname[0], listname[1], listname[2], listname[3]
      return dataset_dict['category_id'][int(category)], listname[0]
    except Exception as ex:
      pass
  files = os.listdir(dataset_path)
  records = []
  for file in files:
    info = parse_into_from_file(file)
    records.append(info)

  df = pd.DataFrame(records)
  df['file'] = files
  df.columns = ['category', 'category_id',  'file']

  return df


df = parse_dataset(dataset_folder_name)
df.head()

# data visualization
import plotly.graph_objects as go

def plot_distribution(pd_series):
  labels = pd_series.value_counts().index.tolist()
  counts = pd_series.value_counts().values.tolist()

  pie_plot = go.Pie(values=counts, labels=labels, hole=0.5)
  fig = go.Figure(data = [pie_plot])
  fig.update_layout(title_text = 'Distribution for %s' %pd_series.name)

  fig.show()

# plot_distribution(df['category'])


# data generator
"""
In order to input data to our Keras multi-output model, we will create a helper 
object to wr=ork as a data generator for out dataset. 
"""


class DataGenerator():
  """
  Data generator for this dataset. This class should be used when training 
  Keras multi-output model
  """
  def __init__(self, df):
    self.df = df
  
  def generate_split_indexes(self):
    p = np.random.permutation(len(self.df))
    train_up_to = int(len(self.df) * train_test_split)
    train_idx = p[:train_up_to]
    test_idx = p[train_up_to:]

    train_up_to = int(train_up_to * train_test_split)
    train_idx, valid_idx = train_idx[:train_up_to], train_idx[train_up_to:]

    return train_idx, valid_idx, test_idx
  

  def preprocessing_image(self, img_path):
    """
    Use to perform some minor preprocessing on the image before inputting into the network
    """
    im = Image.open(img_path).convert('RGB')
    im = im.resize((image_width, image_height))

    im = np.array(im) / 255.0

    return im
    
# using batch_size = 16 , sau nay thi co the su dung batch size khac de huan luyen model
  def generate_images(self, image_idx, is_training, batch_size = 16):
    """
    Using to generate a batch with images when training/ testing/ validating our Keras model
    """

    images, categories= [],  []
    while True:
      for idx in image_idx:
        # đoạn này là lấy hẳn các properties của một ảnh ra này 
          cloth = self.df.iloc[idx]          
          category = cloth['category_id']
          file = cloth['file']

          im = self.preprocessing_image(file)
          try:
            if int(category) <= max(dataset_dict['category_id']) and int(category) >= 0:
              categories.append(to_categorical(category,len(dataset_dict['category_id'])).astype("uint8"))

              images.append(im)
            else :
              continue
          except Exception as err:
            print(error) 



          if len(images) >= batch_size:
            yield shuffle(np.array(images), np.array(categories))
            
            images, categories = [],  []

      if not is_training:
        break

data_generator = DataGenerator(df)
train_idx, valid_idx, test_idx = data_generator.generate_split_indexes()


model = Sequential()
model.add(Conv2D(32, (3,3), input_shape = (image_height,image_width,3), activation = "relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(64, (3,3), activation = "relu"))
model.add(MaxPooling2D(pool_size=(2,2)))
model.add(Dropout(0.25))

model.add(Conv2D(128, (3,3), activation = "relu"))
model.add(Dropout(0.4))

model.add(Flatten())
model.add(Dense(128, activation = "relu"))
model.add(Dropout(0.3))
model.add(Dense(7, activation = 'softmax', name = 'output'))
model.summary()

#   # def build_isAdult_branch(self, input):
#   #   x = Conv2D(32, (3,3), input_shape = (100,100,3), activation = "relu")(input)
#   #   x = MaxPooling2D(pool_size=(2,2),strides=(1,1))(x)

#   #   x = Conv2D(32, (3,3), input_shape = (100,100,3), activation = "relu")(x)
#   #   x = MaxPooling2D(pool_size=(2,2),strides=(1,1))(x)

#   #   x = Conv2D(64, (3,3), input_shape = (100,100,3), activation = "relu")(x)
#   #   x = MaxPooling2D(pool_size=(2,2),strides=(1,1))(x)

#   #   # x.add(Conv2D(32, (3,3)))
#   #   # x.add(Activation('relu'))
#   #   # x.add(MaxPooling2D(pool_size=(2,2)))

#   #   # x.add(Conv2D(64, (3,3)))
#   #   # x.add(Activation('relu'))
#   #   # x.add(MaxPooling2D(pool_size=(2,2)))

#   #   x = Flatten()(x)
#   #   x = Dense(units = 64, activation = "relu")(x)
#   #   x = Dropout(0.5)(x)
#   #   x = Dense(units = 2, activation = 'sigmoid', name = 'adult_out' )(x)

#   #   return x


# # training model 
from tensorflow.keras.optimizers import Adam
# init_lr = 0.01
# epochs = 100

# opt = Adam(learning_rate = init_lr, decay = init_lr/epochs)
# model.compile(optimizer=opt,
#               # loss="categorical_crossentropy",
#               loss_weights={
#                   'output': 0.1,
#                   # 'gender_out': 0.1,
#                   # 'adult_out': 0.1
#               },
#               loss={
#                   'output': 'categorical_crossentropy',
#                   # 'color_out': 'categorical_crossentropy',
#                   # 'gender_out': 'binary_crossentropy',
#                   # 'adult_out': 'binary_crossentropy'
#               },
#               metrics=[tf.keras.metrics.Accuracy()]
#               )


learning_rate = 0.01
epochs = 100

opt = Adam(learning_rate = learning_rate)
model.compile(loss= "categorical_crossentropy",
               optimizer = opt,
               metrics = [tf.keras.metrics.Accuracy()])

from tensorflow.python.util import nest
batch_size = 32
valid_batch_size = 32
train_gen = data_generator.generate_images(train_idx, is_training=True, batch_size = batch_size)
valid_gen = data_generator.generate_images(valid_idx, is_training=True, batch_size = valid_batch_size)


model.fit_generator(
  tuple(train_gen),
    steps_per_epoch=len(train_idx)//batch_size,
    epochs=epochs,
    shuffle=True,
    validation_data=tuple(valid_gen),
    validation_steps=len(valid_idx)//valid_batch_size,
    verbose = 1)
model.save("category.h5")
# accuracy for category
# plt.clf()
# fig = go.Figure()
# fig.add_trace(go.Scatter(
#     y=history.history('output_race'),
#     name='Train'
# ))
# fig.add_trace(go.Scatter(
#     y=history.history('val_output_race'),
#     name='Valid'
# ))
# fig.update_layout(
#     height=500,
#     width=700,
#     title='Accuracy for race feature',
#     xaxis_title='Epoch',
#     yaxis_title='Accuracy'
# )
# fig.show()

# accuracy for color
# plt.clf()
# fig = go.Figure()
# fig.add_trace(go.Scatter(
#     y=history.history('color_output_race'),
#     name='Train'
# ))
# fig.add_trace(go.Scatter(
#     y=history.history('val_color_output_race'),
#     name='Valid'
# ))
# fig.update_layout(
#     height=500,
#     width=700,
#     title='Accuracy for color feature',
#     xaxis_title='Epoch',
#     yaxis_title='Accuracy'
# )
# fig.show()